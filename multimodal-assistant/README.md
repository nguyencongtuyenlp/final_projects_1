# ğŸ¤– Multimodal AI Assistant

**Professional-grade multimodal AI assistant** vá»›i kháº£ nÄƒng xá»­ lÃ½ **text, image, vÃ  audio** trong má»™t há»‡ thá»‘ng thá»‘ng nháº¥t. ÄÆ°á»£c thiáº¿t káº¿ Ä‘á»ƒ production-ready vá»›i kiáº¿n trÃºc sáº¡ch, testing Ä‘áº§y Ä‘á»§, vÃ  deployment dá»… dÃ ng.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.110+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![Tests](https://img.shields.io/badge/Tests-Comprehensive-brightgreen.svg)](./tests/)

## âœ¨ **TÃ­nh nÄƒng chÃ­nh**

### ğŸ”¤ **Text Processing**
- **Summarization**: TÃ³m táº¯t vÄƒn báº£n thÃ´ng minh
- **Question Answering**: Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn context
- **RAG (Retrieval-Augmented Generation)**: TÃ¬m kiáº¿m semantic + QA

### ğŸ‘ï¸ **Vision Processing**  
- **OCR**: TrÃ­ch xuáº¥t text tá»« hÃ¬nh áº£nh (TrOCR)
- **VQA**: Tráº£ lá»i cÃ¢u há»i vá» hÃ¬nh áº£nh (BLIP)
- **Grad-CAM**: Giáº£i thÃ­ch káº¿t quáº£ phÃ¢n loáº¡i áº£nh

### ğŸµ **Audio Processing**
- **ASR**: Chuyá»ƒn giá»ng nÃ³i thÃ nh text (Wav2Vec2)
- **VAD**: PhÃ¡t hiá»‡n hoáº¡t Ä‘á»™ng giá»ng nÃ³i
- **TTS**: Chuyá»ƒn text thÃ nh giá»ng nÃ³i
- **Streaming**: Xá»­ lÃ½ audio realtime qua WebSocket

### ğŸ”¥ **Real-Time Computer Vision** *(NEW!)*
- **Hand Gesture Recognition**: Nháº­n diá»‡n cá»­ chá»‰ tay realtime (MediaPipe)
- **Eye Gaze Tracking**: Theo dÃµi hÆ°á»›ng nhÃ¬n + phÃ¡t hiá»‡n nhÃ¡y máº¯t
- **Face Recognition**: Nháº­n diá»‡n khuÃ´n máº·t tá»« database (optional)
- **WebSocket Streaming**: Xá»­ lÃ½ video frame realtime
- **Web Demo Interface**: Test trá»±c tiáº¿p qua browser

---

## ğŸ—ï¸ **Kiáº¿n trÃºc há»‡ thá»‘ng**

```
multimodal-assistant/
â”œâ”€â”€ app/                          # ğŸš€ FastAPI Application
â”‚   â”œâ”€â”€ pipelines/               # ğŸ”§ AI Processing Pipelines
â”‚   â”‚   â”œâ”€â”€ nlp.py              # Text processing
â”‚   â”‚   â”œâ”€â”€ vision.py           # Image processing  
â”‚   â”‚   â”œâ”€â”€ audio.py            # Audio processing
â”‚   â”‚   â”œâ”€â”€ multimodal.py       # Multimodal orchestration
â”‚   â”‚   â”œâ”€â”€ streaming.py        # Realtime processing
â”‚   â”‚   â””â”€â”€ retrieval.py        # Semantic search
â”‚   â”œâ”€â”€ models/                  # ğŸ¤– Model Management
â”‚   â”‚   â””â”€â”€ registry.py         # Centralized model loading
â”‚   â”œâ”€â”€ services/               # ğŸ”— Business Logic
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Main coordinator
â”‚   â”‚   â”œâ”€â”€ rag_service.py      # RAG implementation
â”‚   â”‚   â””â”€â”€ rag_store.py        # Vector storage
â”‚   â”œâ”€â”€ utils/                  # ğŸ› ï¸ Utilities
â”‚   â”œâ”€â”€ xai/                    # ğŸ” Explainable AI
â”‚   â”œâ”€â”€ schemas.py              # ğŸ“‹ API Schemas
â”‚   â””â”€â”€ server.py               # ğŸŒ FastAPI Server
â”œâ”€â”€ train/                       # ğŸ“ Training Infrastructure
â”œâ”€â”€ tests/                       # ğŸ§ª Comprehensive Testing
â”œâ”€â”€ docker/                      # ğŸ³ Containerization
â””â”€â”€ docs/                        # ğŸ“š Documentation
```

## ğŸš€ **Quick Start**

### **PhÆ°Æ¡ng phÃ¡p 1: Docker (KhuyÃªn dÃ¹ng)**
```bash
# Clone repository
git clone <your-repo-url>
cd multimodal-assistant

# Cháº¡y vá»›i Docker Compose
docker-compose -f docker/docker-compose.yml up --build

# Hoáº·c development mode
docker-compose -f docker/docker-compose.dev.yml up --build
```

### **PhÆ°Æ¡ng phÃ¡p 2: Local Development**
```bash
# Táº¡o virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
make dev-install

# Cháº¡y server
make serve
```

### **Kiá»ƒm tra hoáº¡t Ä‘á»™ng**
```bash
# Health check
curl http://localhost:8000/health

# Swagger UI
open http://localhost:8000/docs

# Real-time demo
open http://localhost:8000/demo
```

### **Real-Time Dependencies**
```bash
# Essential (always install)
pip install mediapipe opencv-python

# Optional (for face recognition - requires CMake on Windows)
pip install face-recognition
```

---

## ğŸ“– **HÆ°á»›ng dáº«n sá»­ dá»¥ng**

### **1. Multimodal Analysis**
```bash
# PhÃ¢n tÃ­ch text + image
curl -X POST "http://localhost:8000/v1/multimodal/analyze" \
  -F 'text=What is in this image?' \
  -F 'tasks=["ocr", "vqa"]' \
  -F 'image=@sample.jpg'
```

### **2. RAG Document Processing**
```bash
# Upload documents
curl -X POST "http://localhost:8000/v1/rag/upload" \
  -F "files=@document.pdf" \
  -F "files=@image.png"

# Query knowledge base  
curl -X POST "http://localhost:8000/v1/rag/query" \
  -d "question=What is the main topic?" \
  -d "top_k=5"
```

### **3. Audio Processing**
```bash
# Speech-to-text
curl -X POST "http://localhost:8000/v1/audio/asr" \
  -F "audio=@speech.wav"

# Text-to-speech
curl -X POST "http://localhost:8000/v1/audio/tts" \
  -d "text=Hello world" \
  --output speech.wav

# Voice activity detection
curl -X POST "http://localhost:8000/v1/audio/vad" \
  -F "audio=@audio.wav"
```

### **4. Explainable AI**
```bash
# Generate Grad-CAM visualization
curl -X POST "http://localhost:8000/v1/xai/gradcam" \
  -F "image=@image.jpg" \
  --output gradcam.png
```

### **5. Real-Time Computer Vision** ğŸ”¥
```bash
# Open web demo interface
open http://localhost:8000/demo

# WebSocket endpoints:
# - ws://localhost:8000/v1/realtime/gesture (Hand gestures)
# - ws://localhost:8000/v1/realtime/eyetracking (Eye gaze)
# - ws://localhost:8000/v1/realtime/vision (Combined)
```

**Web Demo Features:**
- ğŸ“¹ **Live camera feed** with real-time processing
- ğŸ–ï¸ **Hand gestures**: Fist, Thumbs Up, Peace, Open Palm
- ğŸ‘ï¸ **Eye tracking**: Gaze direction + blink detection
- ğŸ“Š **Live statistics** and confidence scores
- ğŸ¯ **Processed images** with landmark visualization

### **6. Realtime WebSocket**
```python
import asyncio
import websockets

async def stream_audio():
    uri = "ws://localhost:8000/v1/realtime/audio"
    async with websockets.connect(uri) as ws:
        # Send audio chunks
        await ws.send(audio_bytes)
        
        # Receive transcription
        result = await ws.recv()
        print(result)
```

**ğŸ“‹ Xem [API_DOCS.md](./API_DOCS.md) Ä‘á»ƒ biáº¿t chi tiáº¿t Ä‘áº§y Ä‘á»§ vá» cÃ¡c endpoints.**

## ğŸ§ª **Development & Testing**

### **Testing**
```bash
# Cháº¡y toÃ n bá»™ tests
make test

# Test vá»›i coverage report
make test-cov

# Test specific module
pytest tests/test_pipelines.py -v

# Test vá»›i watch mode (tá»± Ä‘á»™ng re-run khi code thay Ä‘á»•i)
make test-watch
```

### **Code Quality**
```bash
# Format code
make fmt

# Type checking
mypy app/

# Security scan
bandit -r app/
```

### **Development Commands**
```bash
# Xem táº¥t cáº£ commands
make help

# Clean cache files
make clean

# Install development dependencies
make dev-install
```

---

## ğŸ³ **Deployment**

### **Production vá»›i Docker**
```bash
# Build production image
docker build -f docker/Dockerfile -t multimodal-assistant .

# Run production container
docker run -p 8000:8000 \
  -v $(pwd)/storage:/app/storage \
  -e APP_ENV=production \
  multimodal-assistant
```

### **Environment Variables**
```bash
# Core settings
APP_ENV=production          # development/production
APP_HOST=0.0.0.0           # Host to bind
APP_PORT=8000              # Port to listen

# Optional authentication
APP_AUTH_TOKEN=your_secret_token

# Model caching
TRANSFORMERS_CACHE=/path/to/model/cache
```

### **Health Monitoring**
```bash
# Health check endpoint
curl http://localhost:8000/health

# Metrics (planning)
curl http://localhost:8000/metrics
```

---

## ğŸ¤– **Models & AI Components**

### **Pre-trained Models**
| Component | Model | Size | Purpose |
|-----------|-------|------|---------|
| OCR | microsoft/trocr-base-printed | ~558MB | Text extraction |
| VQA | Salesforce/blip-vqa-base | ~990MB | Visual Q&A |
| ASR | facebook/wav2vec2-base-960h | ~360MB | Speech-to-text |
| Summarization | sshleifer/distilbart-cnn-12-6 | ~306MB | Text summarization |
| QA | deepset/bert-base-cased-squad2 | ~436MB | Question answering |
| Embeddings | sentence-transformers/all-MiniLM-L6-v2 | ~90MB | Semantic search |

### **Model Loading**
- Models tá»± Ä‘á»™ng download khi cháº¡y láº§n Ä‘áº§u
- Cached locally Ä‘á»ƒ sá»­ dá»¥ng tiáº¿p theo
- Sá»­ dá»¥ng `@lru_cache` Ä‘á»ƒ memory efficiency

---

## ğŸ”® **Roadmap**

### **Phase 1 (Completed) âœ…**
- [x] Multimodal API endpoints
- [x] RAG document processing  
- [x] Audio processing (ASR, TTS, VAD)
- [x] Realtime WebSocket streaming
- [x] Comprehensive testing
- [x] Docker deployment

### **Phase 2 (Planning) ğŸš§**
- [ ] Model fine-tuning infrastructure
- [ ] Batch processing endpoints
- [ ] Advanced caching (Redis)
- [ ] Performance optimization
- [ ] Monitoring & metrics

### **Phase 3 (Future) ğŸŒŸ**  
- [ ] Multi-language support
- [ ] Custom model training UI
- [ ] Advanced XAI features
- [ ] Kubernetes deployment
- [ ] Auto-scaling capabilities

---

## ğŸ¤ **Contributing**

1. Fork repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### **Development Setup**
```bash
# Clone vÃ  setup
git clone <repo-url>
cd multimodal-assistant
make dev-install

# Pre-commit hooks
pre-commit install

# Run tests trÆ°á»›c khi commit
make test
```

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ **Acknowledgments**

- [Hugging Face](https://huggingface.co/) cho pre-trained models
- [FastAPI](https://fastapi.tiangolo.com/) cho web framework
- [PyTorch](https://pytorch.org/) cho deep learning infrastructure.
